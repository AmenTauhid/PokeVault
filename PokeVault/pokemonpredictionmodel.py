# -*- coding: utf-8 -*-
"""PokemonPredictionModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18MvOePyqTdChxpvQN6EvBQvcNXNaVBqV
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

# Set random seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# Load dataset
file_path = 'pokemon.csv'  # Adjust path as needed
df = pd.read_csv(file_path)

# Display basic information about the dataset
print(f"Dataset shape: {df.shape}")
print("\nFirst few rows:")
print(df.head())

# Data preparation
print("\nChecking for missing values:")
print(df.isnull().sum())

# Examine columns with missing values more carefully
numeric_cols = df.select_dtypes(include=[np.number]).columns
print("\nColumns with NaN values in numeric columns:")
for col in numeric_cols:
    if df[col].isnull().sum() > 0:
        print(f"{col}: {df[col].isnull().sum()} missing values")

# Handle missing values
df['type2'].fillna('none', inplace=True)  # Fill missing type2 with 'none'

# Fix datatypes and missing values for all numeric columns
df['capture_rate'] = pd.to_numeric(df['capture_rate'], errors='coerce')
df['percentage_male'] = pd.to_numeric(df['percentage_male'], errors='coerce')
df['height_m'] = pd.to_numeric(df['height_m'], errors='coerce')
df['weight_kg'] = pd.to_numeric(df['weight_kg'], errors='coerce')

# Fill missing values in numeric columns with medians
numeric_cols = df.select_dtypes(include=[np.number]).columns
for col in numeric_cols:
    if df[col].isnull().sum() > 0:
        print(f"Filling {df[col].isnull().sum()} missing values in {col} with median")
        df[col].fillna(df[col].median(), inplace=True)

# Feature engineering: Create a more comprehensive pricing model
# -------------------------------------------------------------

# 1. Create rarity score based on capture rate (lower rate = higher rarity)
df['rarity_score'] = 255 / (df['capture_rate'] + 1)  # Add 1 to avoid division by zero

# 2. Create type popularity score (based on fan popularity - approximated)
type_popularity = {
    'dragon': 10, 'fire': 9, 'water': 8, 'electric': 8, 'psychic': 7.5,
    'ghost': 7.5, 'dark': 7, 'fairy': 7, 'steel': 6.5, 'ice': 6,
    'fighting': 5.5, 'ground': 5, 'poison': 5, 'flying': 5,
    'rock': 4.5, 'grass': 4.5, 'bug': 3.5, 'normal': 3, 'none': 0
}
df['type1_popularity'] = df['type1'].map(type_popularity)
df['type2_popularity'] = df['type2'].map(type_popularity).fillna(0)
df['type_score'] = df['type1_popularity'] * 0.7 + df['type2_popularity'] * 0.3

# 3. Generation score (newer generations might have higher collectible value initially)
gen_multiplier = {1: 1.2, 2: 0.9, 3: 0.95, 4: 1.0, 5: 1.1, 6: 1.15, 7: 1.2}
df['gen_score'] = df['generation'].map(gen_multiplier)

# 4. Create a base power score
df['power_score'] = (
    df['hp'] * 0.08 +
    df['attack'] * 0.15 +
    df['defense'] * 0.12 +
    df['sp_attack'] * 0.15 +
    df['sp_defense'] * 0.12 +
    df['speed'] * 0.13 +
    df['base_total'] * 0.25
)

# 5. Starter bonus (Pokemon that are typically starters tend to be more popular)
starter_names = ['Bulbasaur', 'Ivysaur', 'Venusaur', 'Charmander', 'Charmeleon', 'Charizard',
                 'Squirtle', 'Wartortle', 'Blastoise', 'Chikorita', 'Bayleef', 'Meganium',
                 'Cyndaquil', 'Quilava', 'Typhlosion', 'Totodile', 'Croconaw', 'Feraligatr',
                 'Treecko', 'Grovyle', 'Sceptile', 'Torchic', 'Combusken', 'Blaziken',
                 'Mudkip', 'Marshtomp', 'Swampert', 'Turtwig', 'Grotle', 'Torterra',
                 'Chimchar', 'Monferno', 'Infernape', 'Piplup', 'Prinplup', 'Empoleon',
                 'Snivy', 'Servine', 'Serperior', 'Tepig', 'Pignite', 'Emboar',
                 'Oshawott', 'Dewott', 'Samurott', 'Chespin', 'Quilladin', 'Chesnaught',
                 'Fennekin', 'Braixen', 'Delphox', 'Froakie', 'Frogadier', 'Greninja',
                 'Rowlet', 'Dartrix', 'Decidueye', 'Litten', 'Torracat', 'Incineroar',
                 'Popplio', 'Brionne', 'Primarina', 'Pikachu', 'Raichu', 'Eevee']
df['is_starter'] = df['name'].isin(starter_names).astype(int)

# 6. Mascot bonus (Pokemon that are faces of the franchise)
mascot_names = ['Pikachu', 'Charizard', 'Mewtwo', 'Lucario', 'Greninja', 'Eevee', 'Jigglypuff']
df['is_mascot'] = df['name'].isin(mascot_names).astype(int)

# 7. Calculate a desirability score (rarity + power + popularity)
df['desirability_score'] = (
    df['rarity_score'] * 0.2 +
    df['power_score'] * 0.3 +
    df['type_score'] * 0.1 +
    df['is_legendary'] * 50 +  # Legendary status is a huge booster
    df['is_starter'] * 15 +    # Starter bonus
    df['is_mascot'] * 30       # Mascot bonus
) * df['gen_score']            # Apply generation multiplier

# 8. Convert desirability score to a price (in dollars)
# Base price range from $0.50 (common) to ~$1000 (rarest legendaries)
min_price = 0.5
max_price = 1000
df['price'] = min_price + df['desirability_score'] * (max_price - min_price) / df['desirability_score'].max()
df['price'] = df['price'].round(2)  # Round to 2 decimal places

# Print summary statistics for our generated price
print("\nGenerated Price Summary Statistics:")
print(df['price'].describe())

# Visualize price distribution
plt.figure(figsize=(12, 6))
sns.histplot(df['price'], bins=30, kde=True)
plt.title('Distribution of Generated Pokémon Prices')
plt.xlabel('Price (USD)')
plt.ylabel('Count')
plt.tight_layout()
plt.savefig('price_distribution.png')

# Visualize price by type
plt.figure(figsize=(14, 8))
sns.boxplot(x='type1', y='price', data=df)
plt.title('Price Distribution by Primary Type')
plt.xlabel('Primary Type')
plt.ylabel('Price (USD)')
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('price_by_type.png')

# Visualize legendary status impact
plt.figure(figsize=(10, 6))
sns.boxplot(x='is_legendary', y='price', data=df)
plt.title('Price Distribution by Legendary Status')
plt.xlabel('Is Legendary (0 = No, 1 = Yes)')
plt.ylabel('Price (USD)')
plt.tight_layout()
plt.savefig('price_by_legendary.png')

# Feature selection for the ML model
features = [
    'hp', 'attack', 'defense', 'sp_attack', 'sp_defense', 'speed',
    'base_total', 'base_egg_steps', 'experience_growth', 'base_happiness',
    'capture_rate', 'height_m', 'weight_kg', 'generation', 'is_legendary',
    'percentage_male', 'rarity_score', 'type_score', 'gen_score',
    'is_starter', 'is_mascot'
]

# One-hot encode the types
types1 = pd.get_dummies(df['type1'], prefix='type1')
types2 = pd.get_dummies(df['type2'], prefix='type2')

# Combine features
X = pd.concat([df[features], types1, types2], axis=1)
y = df['price']

# Print the final feature list
print("\nFinal features for the model:")
print(X.columns.tolist())
print(f"Total number of features: {X.shape[1]}")

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Build the TensorFlow model with improved architecture
model = keras.Sequential([
    keras.layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    keras.layers.BatchNormalization(),
    keras.layers.Dropout(0.3),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.BatchNormalization(),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(1)  # Output layer for regression
])

# Compile with appropriate loss function for price prediction
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    loss='mse',  # Mean Squared Error for regression
    metrics=['mae', 'mse']  # Mean Absolute Error and Mean Squared Error
)

# Add early stopping to prevent overfitting
early_stopping = keras.callbacks.EarlyStopping(
    monitor='val_mae',
    patience=15,
    restore_best_weights=True
)

# Train the model with more epochs but early stopping
epochs = 100
history = model.fit(
    X_train_scaled, y_train,
    validation_data=(X_test_scaled, y_test),
    epochs=epochs,
    batch_size=32,
    callbacks=[early_stopping],
    verbose=1
)

# Plot training history
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['mae'], label='Train MAE')
plt.plot(history.history['val_mae'], label='Validation MAE')
plt.xlabel('Epochs')
plt.ylabel('Mean Absolute Error')
plt.legend()
plt.title('MAE During Training')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss (MSE)')
plt.legend()
plt.title('Loss During Training')

plt.tight_layout()
plt.savefig('training_history.png')

# Evaluate the model
test_loss, test_mae, test_mse = model.evaluate(X_test_scaled, y_test, verbose=0)
print(f"\nTest MAE: ${test_mae:.2f}")
print(f"Test RMSE: ${np.sqrt(test_mse):.2f}")

# Make predictions on the test set
y_pred = model.predict(X_test_scaled).flatten()

# Plot actual vs predicted prices
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.xlabel('Actual Price ($)')
plt.ylabel('Predicted Price ($)')
plt.title('Actual vs. Predicted Pokémon Prices')
plt.tight_layout()
plt.savefig('actual_vs_predicted.png')

# Get feature importance (indirectly through a simple linear model)
from sklearn.linear_model import LinearRegression
from sklearn.impute import SimpleImputer

# First, check for any NaN values in our features
print("\nChecking for NaN values in features:")
print(X_train.isna().sum().sum(), "NaN values found in X_train")

# Create imputer to handle NaN values
imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)

# Now fit the linear model on imputed data
linear_model = LinearRegression()
linear_model.fit(X_train_imputed, y_train)
importance = np.abs(linear_model.coef_)
feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': importance})
feature_importance = feature_importance.sort_values('Importance', ascending=False)

# Plot top 20 feature importances
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=feature_importance.head(20))
plt.title('Top 20 Features for Pokémon Price Prediction')
plt.tight_layout()
plt.savefig('feature_importance.png')

# Predict prices for all Pokémon using our model
# First ensure there are no NaNs in the data for prediction
X_imputed = imputer.transform(X)
df['Predicted_Price'] = model.predict(scaler.transform(X_imputed)).flatten()

# Show Pokémon with their predicted prices
result_df = df[['name', 'type1', 'type2', 'base_total', 'is_legendary', 'price', 'Predicted_Price']]
result_df = result_df.sort_values('Predicted_Price', ascending=False)

print("\nTop 20 Most Valuable Pokémon:")
print(result_df.head(20))

print("\nLeast Valuable Pokémon:")
print(result_df.tail(10))

# Save predictions to CSV
output_file = 'pokemon_with_prices.csv'
result_df.to_csv(output_file, index=False)
print(f"\nPredictions saved to {output_file}")

# Create a scatter plot to visualize how different factors influence the price
plt.figure(figsize=(12, 8))
scatter = plt.scatter(df['base_total'], df['price'],
                     c=df['is_legendary'], cmap='coolwarm',
                     alpha=0.7, s=df['generation']*20)
plt.xlabel('Base Total Stats')
plt.ylabel('Price ($)')
plt.title('Pokémon Price vs. Base Stats (Color: Legendary Status, Size: Generation)')
plt.colorbar(scatter, label='Is Legendary')
plt.tight_layout()
plt.savefig('price_vs_stats.png')

print("\nAnalysis complete. Images and CSV output have been saved.")